---
uuid: e96328d9-983c-4e36-a51e-3ea1eb4a7846
title: Adapting Large Language Models to Domains via Reading Comprehension
description: この論文では、ドメイン特化の生データを読解テキストに変換する新しい手法を提案し、生物医学、経済、法律などの分野での能力向上と汎用的なプロンプト対応能力の獲得を確認しました。実験結果は、提案手法（AdaptLLM）が他の手法よりも優れた性能を示し、特に生物医学と法律の分野で顕著な向上が見られました。
category: paper-stream
tags:
  - LLM
  - Pre-train
  - arxiv
  - ICLR
lang: ja
created_at: 2024-07-09
updated_at: 2024-09-20
---

## AI要約

この論文は、ドメイン特化の生データを読解テキストに変換する新しい方法を提案しています。この方法により、生物医学、経済、法律など特定の分野における能力の向上と、汎用的なプロンプト対応能力の獲得が確認されました。具体的には、通常の継続事前学習(DAPT)ではプロンプト能力が低下することがある一方で、提案された読解テキストを用いる方法が最も良い結果を示しました。

実験結果によれば、読解テキストを用いることで、ドメイン知識の習得とプロンプト解釈能力の両方が向上します。これは、ドメイン特有のコーパスフォーマットの多様性が少ないことに起因する問題を解決するためのアプローチとして有効であることが示されています。具体的なタスクとしては、要約、単語変換、自然言語推論、常識的な推論、パラフレーズ検出、テキスト補完などが含まれます。

さらに、正規表現を用いたタスク評価や一般的な指示学習データセット(LIMA, WizardLM, Orca)を用いた学習も行われています。メインの結果としては、提案手法(AdaptLLM)が他の手法よりも良い結果を示しており、特定のタスクにおいても優れた性能を発揮しています。アブレーションスタディでも、読解テキストを用いる手法が最も良い結果を示しており、ドメイン知識とプロンプト解釈能力の両方が向上することが確認されました。

最後に、FLANデータセットを用いて評価を行った結果、読解テキストを用いるとプロンプト能力が大幅に向上することが示されています。特に、生物医学と法律の分野では、読解テキストを使用することで大幅な性能向上が見られました。

## この論文で述べられていること

- domain specificなraw corporaで学習すると、ドメイン知識は習得できるが、汎用的なプロンプトへの対応能力がなくなる
- 本論文では、raw corporaを読解テキストに変換する方法を提案する
- この方法で、生物医学、経済、法律分野での能力向上と汎用的なプロンプトへの対応能力の獲得が認められた

## 課題感

LLaMaを使った実験。DAPT (通常の継続事前学習) を行うと、Prompting能力が下がってしまうことがある。単純にPromptを与えて解いた時と、それぞれの問題にFinetuneした時の結果の比較。

![160d6659-1ae2-497e-81a3-5c2f3a908003](../public/paperStream/160d6659-1ae2-497e-81a3-5c2f3a908003.png)

これはドメイン特有のコーパスの中にはフォーマットの多様性がないことに起因するのではないかという仮説。

### Domain specific corpus

![7003e67d-d361-473a-a2a4-550b0c55ba7a](../public/paperStream/7003e67d-d361-473a-a2a4-550b0c55ba7a.png)

## 読解テキストへの変換

![dfb7e46b-14ea-422e-a80f-0114f4b85290](../public/paperStream/dfb7e46b-14ea-422e-a80f-0114f4b85290.png)

Raw textを以下のTaskごとにそれぞれの生成を行い、Raw textの後に繋げる。繋げたことを示すために `\n\n` を入れる。この生成は完全にルールベースで行われる (LLMは用いない)。

c.f. [https://github.com/microsoft/LMOps/blob/main/adaptllm/utils/read.py](https://github.com/microsoft/LMOps/blob/main/adaptllm/utils/read.py)

- Summarization
- Word to text
  - ドメイン特有のwordの判定は、generalなtextとdomain specific
  - なデータから作られたtokenizer (sentence piece)を比較して、generalなtextに含まれていない10文字以上の単語を選択する。
- Natural Language Inference
- Commonsense Reasoning
- Paraphrase Detection
- Text Completion
  - LLMではなく、半分を切って、その半分に何が続くかを作成する

  ```javascript
  "Please complete an article about {domain}: {context_1st_half} {context_2nd_half}{qa_demos}";
  ```

### Examples

![461443ee-0d0c-4ce0-81fd-964238a714d7](../public/paperStream/461443ee-0d0c-4ce0-81fd-964238a714d7.png)

## 実装の詳細

正規表現を使ってそれぞれのタスクがraw corpusに含まれているか評価し、含まれている場合規定の言い換えを行う。

SENTxは文章、VERBALは正規表現の表に入っているもの、WORDはなんらかの単語

![fb2b003d-42c6-4b3a-b818-75a7ff4bdb9c](../public/paperStream/fb2b003d-42c6-4b3a-b818-75a7ff4bdb9c.png)

正規表現は以下の表のようになっている。

![46ffe906-642f-4e28-ab3c-aabed3c7c6f0](../public/paperStream/46ffe906-642f-4e28-ab3c-aabed3c7c6f0.png)

## General Instructions

[LIMA](https://arxiv.org/abs/2305.11206), [WizardLM](https://arxiv.org/abs/2304.12244), [Orca](https://huggingface.co/datasets/Open-Orca/OpenOrca)のデータセットを用いて一般的な指示学習を行っている。

## Main Results

General Instruction + 読解テキストを用いた学習を評価している。比較対象は、ベースモデル、存在する場合はベースモデルから作成された特化モデル、raw corpusを使って訓練されたモデル、提案手法。

![f795ec22-91d4-4194-983d-4f7e53850c3b](../public/paperStream/f795ec22-91d4-4194-983d-4f7e53850c3b.png)

AVERAGEでは基本的にAdaptLLM (今回の手法を使って訓練したもの) が良い結果を収めている。必ずしもそれぞれのタスクで全てよくなっているというわけではない。例えばUMSLEはよくなってないが、UMSLEが上がらないのはコーパスがPubmedだからな気もする。

DAPTは普通の継続学習をしたもの。ベースモデルはそれぞれのセクションの一番上

### Task詳細

![80179c7c-40b7-4aa3-ac12-6985deb30d70](../public/paperStream/80179c7c-40b7-4aa3-ac12-6985deb30d70.png)

## Ablation study

![d87c872b-4c43-4743-b15d-4aafc267c439](../public/paperStream/d87c872b-4c43-4743-b15d-4aafc267c439.png)

提案手法が一番良い結果を示している。Gen. Insが結構強く見えるが、Rawと比べると一応上昇しているので良いのか。少なくとも相乗的な効果はある。

## ドメイン知識とプロンプトの解釈能力

![93ca782b-44b1-4f22-8b4a-58900fdaf590](../public/paperStream/93ca782b-44b1-4f22-8b4a-58900fdaf590.png)

### Domain Knowledgeの評価

右の図は以下の表の平均を表している。基本的に読解テキストを使うのが一番良くなっている。

![9e1e1637-c7fd-4d65-8beb-51bf207273c0](../public/paperStream/9e1e1637-c7fd-4d65-8beb-51bf207273c0.png)

### Domain knowledge probing

多肢問題のtoken probabilityを見ている。

Bio: MedMCQA

Law: LEDGAR dataset

### Prompt ability

FLANデータセットを使って評価。

- Raw textだとGeneral LLMより悪くなる部分がある (Read Compre, Close QA, Paraphrase, NLI, Common Reason)。
- 読解テキストを使うと、Prompt abilityもGeneral LLMより大幅に伸びていたりする(Paraphraseだけ若干悪くなっている)。
  - Ablation studyがあって、Paraphraseを外すと性能が上がる
  - ただしDomain taskにはparaphraseも寄与している

  ![9ae8dcb0-d72c-44af-b663-d2d06dcfaa3d](../public/paperStream/9ae8dcb0-d72c-44af-b663-d2d06dcfaa3d.png)

実際の結果は以下の表 (Appendix E)

![02ddc424-368b-4197-a7ae-ae871f1727ed](../public/paperStream/02ddc424-368b-4197-a7ae-ae871f1727ed.png)

## 関連ページ

- [open review](https://openreview.net/forum?id=y886UXPEZ0)
- [Github](https://github.com/microsoft/LMOps)
